# Scalable Bayesian Inference via Stochastic Gradient Langevin Dynamics

This project investigates scalable Bayesian inference using Stochastic Gradient Langevin Dynamics (SGLD). 
The aim is to compare SGLD with classical MCMC methods and evaluate its effectiveness for uncertainty-aware learning on larger datasets.
This mini-project was completed during the Michaelmas term as part of the Machine Learning module in my MSc programme.

***
1-Analytical vs Numerical Temperature Profiles

We compared the analytical Stefan solution with numerical simulations of the phase-field model. The numerical solution (dashed) matches the analytical solution (solid) closely. The small discrepancy at the interface is due to the phase-field’s “smoothing” effect.
<img width="800" height="600" alt="Temperature_Profile" src="https://github.com/user-attachments/assets/f4842c1e-7e59-4613-89a6-9dadebac84c3" />


***
2–Asymptotic Expansion of Initial Conditions
